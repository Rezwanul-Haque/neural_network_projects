{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors       | Email                  |\n",
    ":--------------|:----------------------|\n",
    "Rezwanul      | rezwanul.cse@gmail.com |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Sentiment Analysis of Movie Reviews Using LSTM.\n",
    "### Problem class: NLP, Sequential\n",
    "\n",
    "#### The IMDb dataset is provided by IMDb.\n",
    "### Problem dataset link: [IMDb dataset](https://www.imdb.com/)\n",
    "\n",
    "### Problem description:\n",
    "\n",
    " Predicting the sentiment of movie reviews on IMDb.\n",
    "\n",
    "### Problem Task:  Build and train a LSTM network to predict the sentiment of movie reviews on IMDb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import piexif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, mean_squared_error\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, testing_set = imdb.load_data(index_from=3)\n",
    "X_train, y_train = training_set\n",
    "X_test, y_test = testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# Print the first movie\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As dataset already encoded the words as number by keras we can decode it using keras word-to-index\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = { key: (value+3) for key, value in word_to_id.items()}\n",
    "word_to_id[\"<START>\"] = 1\n",
    "id_to_word = { value: key for key, value in word_to_id.items() }\n",
    "\n",
    "#Insight: Clearly, the sentiment of this review is negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> a rating of 1 does not begin to express how dull depressing and relentlessly bad this movie is\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_to_word[id] for id in X_train[159])) ## 159 number review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[159])\n",
    "\n",
    "## Insight: A y value of 0 refers to a negative review and a y value of 1 refers to a positive review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> lavish production values and solid performances in this straightforward adaption of jane austen's satirical classic about the marriage game within and between the classes in provincial 18th century england northam and paltrow are a salutory mixture as friends who must pass through jealousies and lies to discover that they love each other good humor is a sustaining virtue which goes a long way towards explaining the accessability of the aged source material which has been toned down a bit in its harsh scepticism i liked the look of the film and how shots were set up and i thought it didn't rely too much on successions of head shots like most other films of the 80s and 90s do very good results\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_to_word[id] for id in X_train[6])) ## 6 number review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train[6])\n",
    "\n",
    "## Insight: y value of 1 refers to a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding cannot be use to encode the data so we can use word embeddings\n",
    "## Word embeddings are a learned form of vector representation for words. The\n",
    "## main advantage of word embeddings is that they have fewer dimensions than\n",
    "## the one-hot encoded representation, and they place similar words close to one\n",
    "## another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word embeddings](images/word_embeddings.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the learned word embedding knows that the words \"Elated\",\n",
    "# \"Happy\", and \"Excited\" are similar words, and hence should be placed near\n",
    "# each other. Similarly, the words \"Sad\", \"Disappointed\", \"Angry\", and\n",
    "# \"Furious\" are on the opposite ends of the spectrum, and should be placed far\n",
    "# away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model architecture](images/model_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, testing_set = imdb.load_data(num_words=10000)\n",
    "# num_words: This is defined as the maximum number of unique words to be\n",
    "# loaded. Only the n most common unique words (as they appear in the\n",
    "# dataset) will be loaded. If n is small, the training time will be faster at the\n",
    "# expense of accuracy.\n",
    "X_train, y_train = training_set\n",
    "X_test, y_test = testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 25000\n",
      "Number of testing samples = 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples = {X_train.shape[0]}\")\n",
    "print(f\"Number of testing samples = {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero padding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie reviews have different lengths, and therefore the input vectors have different sizes.\n",
    "# This is an issue, as neural networks only accept fixed-size vectors.\n",
    "\n",
    "# Define a maxlen parameter. The maxlen parameter shall be the maximum length of each movie review.\n",
    "# Reviews that are longer than maxlen will be truncated, and reviews that are shorter than maxlen will be padded with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![zero padding](images/zero_padding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train vector shape = (25000, 100)\n",
      "X_test vector shape = (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train vector shape = {X_train_padded.shape}\")\n",
    "print(f\"X_test vector shape = {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rezwanul/anaconda3/envs/nn-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note:\n",
    "# The Embedding class takes the following important arguments:\n",
    "# input_dim: The input dimensions of the word embedding layer. This\n",
    "# should be the same as the num_words parameter that we used when we\n",
    "# loaded in our data. Essentially, this is the maximum number of unique\n",
    "# words in our dataset.\n",
    "# output_dim: The output dimensions of the word embedding layer. This\n",
    "# should be a hyperparameter to be fine-tuned. For now, we can use a value\n",
    "# of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rezwanul/anaconda3/envs/nn-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rezwanul/anaconda3/envs/nn-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(input_dim=10000, output_dim=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: \n",
    "# The LSTM class takes the following important arguments:\n",
    "# # units: This refers to the number of recurring units in the LSTM layer. A\n",
    "# larger number of units results in a more complex model, at the expense\n",
    "# of training time and overfitting. For now, let's use a typical value of 128\n",
    "# for the number of units.\n",
    "# # activation: This refers to the type of activation function applied to the cell\n",
    "# state and the hidden state. The default value is the tanh function.\n",
    "# # recurrent_activation: This refers to the type of activation function applied\n",
    "# to the forget, input, and output gates. The default value is the sigmoid\n",
    "# function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(units=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,411,713\n",
      "Trainable params: 1,411,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the SGD optimizer as for LSTM, it is often impossible to know beforehand which optimizer\n",
    "# will be better for the dataset\n",
    "Optimizers = ['SGD', 'RMSprop', 'adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rezwanul/anaconda3/envs/nn-env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rezwanul/anaconda3/envs/nn-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rezwanul/anaconda3/envs/nn-env/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Optimizers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = model.fit(x=X_train_padded, y=y_train, batch_size=128, epochs=10, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Optimizer, X_train, y_train, X_val, y_val):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128))\n",
    "    model.add(LSTM(units=128))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Optimizer, metrics=['accuracy'])\n",
    "    scores = model.fit(x=X_train, y=y_train, batch_size=128, epochs=10, validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    return scores, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_score, SGD_model = train_model(Optimizer='sgd', \n",
    "                                   X_train=X_train_padded,\n",
    "                                   y_train=y_train,\n",
    "                                   X_val=X_test_padded,\n",
    "                                   y_val=y_test\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSprop_score, RMSprop_model = train_model(Optimizer='RMSprop', \n",
    "                                   X_train=X_train_padded,\n",
    "                                   y_train=y_train,\n",
    "                                   X_val=X_test_padded,\n",
    "                                   y_val=y_test\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam_score, Adam_model = train_model(Optimizer='adam', \n",
    "                                   X_train=X_train_padded,\n",
    "                                   y_train=y_train,\n",
    "                                   X_val=X_test_padded,\n",
    "                                   y_val=y_test\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
